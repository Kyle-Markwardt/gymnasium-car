{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 14:38:16.655816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 777\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "batch_size = 32\n",
    "max_steps_per_episode = 5000\n",
    "target_update_freq = 5000  # More frequent updates for better monitoring\n",
    "replay_buffer_size = 100000\n",
    "epsilon_random_frames = 50000\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "update_after_actions = 4\n",
    "checkpoint_freq = 10  # Save model every 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrappers for environment\n",
    "\n",
    "# Because Car Racing has continuous actions (steer, gas, brake) this ensures action space is noramilzed \n",
    "class NormalizeActionWrapper(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        action = np.clip(action, -1, 1)\n",
    "        return action\n",
    "\n",
    "# Sets RGB channels to grayscale (1 v 3 channels) and reseizes from 96x96 to 84x84 for faster processing\n",
    "class ResizeAndGrayscaleWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ResizeAndGrayscaleWrapper, self).__init__(env)\n",
    "        from gym.spaces import Box\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        gray_observation = rgb2gray(observation)\n",
    "        resized_observation = resize(gray_observation, (84, 84), anti_aliasing=True)\n",
    "        return np.expand_dims(resized_observation, axis=-1)\n",
    "\n",
    "def make_carracing_env(seed):\n",
    "    env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "    env = ResizeAndGrayscaleWrapper(env)\n",
    "    env = NormalizeActionWrapper(env)\n",
    "    env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "# Create the environment\n",
    "env = make_carracing_env(seed)\n",
    "\n",
    "num_actions = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(84, 84, 1))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to make an action.\n",
    "model = create_q_model()\n",
    "# Build target model for the prediction of future rewards.\n",
    "# The weights of target model get updated every 10000 steps thus when the loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()\n",
    "model_target.set_weights(model.get_weights())  # Initialize target model with same weights as model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward: 0.00 at episode 0, frame count 5000\n",
      "Episode 1/1000 - Reward: -426.0563380281976 - Running Reward: -426.06 - Epsilon: 1.00\n",
      "Running reward: -426.06 at episode 1, frame count 10000\n",
      "Episode 2/1000 - Reward: -435.6913183280052 - Running Reward: -430.87 - Epsilon: 0.99\n",
      "Running reward: -430.87 at episode 2, frame count 15000\n",
      "Episode 3/1000 - Reward: -432.4758842444031 - Running Reward: -431.41 - Epsilon: 0.99\n",
      "Running reward: -431.41 at episode 3, frame count 20000\n",
      "Episode 4/1000 - Reward: -431.8181818182119 - Running Reward: -431.51 - Epsilon: 0.98\n",
      "Episode 5/1000 - Reward: -420.600000000004 - Running Reward: -429.33 - Epsilon: 0.98\n",
      "Running reward: -429.33 at episode 5, frame count 25000\n",
      "Episode 6/1000 - Reward: -433.7349397590668 - Running Reward: -430.06 - Epsilon: 0.97\n",
      "Running reward: -430.06 at episode 6, frame count 30000\n",
      "Episode 7/1000 - Reward: -429.2929292929587 - Running Reward: -429.95 - Epsilon: 0.97\n",
      "Running reward: -429.95 at episode 7, frame count 35000\n",
      "Episode 8/1000 - Reward: -426.31578947371304 - Running Reward: -429.50 - Epsilon: 0.96\n",
      "Running reward: -429.50 at episode 8, frame count 40000\n",
      "Episode 9/1000 - Reward: -430.6930693069606 - Running Reward: -429.63 - Epsilon: 0.96\n",
      "Running reward: -429.63 at episode 9, frame count 45000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/1000 - Reward: -432.25806451615915 - Running Reward: -429.89 - Epsilon: 0.96\n",
      "Checkpoint saved at episode 10\n",
      "Running reward: -429.89 at episode 10, frame count 50000\n",
      "Episode 11/1000 - Reward: -426.31578947371304 - Running Reward: -429.57 - Epsilon: 0.95\n",
      "Running reward: -429.57 at episode 11, frame count 55000\n",
      "Episode 12/1000 - Reward: -433.7539432176962 - Running Reward: -429.92 - Epsilon: 0.95\n",
      "Running reward: -429.92 at episode 12, frame count 60000\n",
      "Episode 13/1000 - Reward: -434.9845201238699 - Running Reward: -430.31 - Epsilon: 0.94\n",
      "Running reward: -430.31 at episode 13, frame count 65000\n",
      "Episode 14/1000 - Reward: -436.74698795183843 - Running Reward: -430.77 - Epsilon: 0.94\n",
      "Running reward: -430.77 at episode 14, frame count 70000\n",
      "Episode 15/1000 - Reward: -429.05405405408357 - Running Reward: -430.65 - Epsilon: 0.93\n",
      "Running reward: -430.65 at episode 15, frame count 75000\n",
      "Episode 16/1000 - Reward: -421.05263157897497 - Running Reward: -430.05 - Epsilon: 0.93\n",
      "Running reward: -430.05 at episode 16, frame count 80000\n",
      "Episode 17/1000 - Reward: -387.53363228699334 - Running Reward: -427.55 - Epsilon: 0.92\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m     batch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(replay_buffer, batch_size)\n\u001b[1;32m     74\u001b[0m     states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(np\u001b[38;5;241m.\u001b[39marray, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))\n\u001b[0;32m---> 76\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_count \u001b[38;5;241m%\u001b[39m target_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     79\u001b[0m     model_target\u001b[38;5;241m.\u001b[39mset_weights(model\u001b[38;5;241m.\u001b[39mget_weights())\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(states, actions, rewards, next_states, dones):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 11\u001b[0m         q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# Get the predicted Q-values for the taken actions\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         q_values_for_actions \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(actions \u001b[38;5;241m*\u001b[39m q_values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rlcar/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:112\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_handler\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_traceback_filtering_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    115\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rlcar/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:82\u001b[0m, in \u001b[0;36mis_traceback_filtering_enabled\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.config.is_traceback_filtering_enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_traceback_filtering_enabled\u001b[39m():\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check if traceback filtering is enabled.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    Raw Keras tracebacks (also known as stack traces)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m        and `False` otherwise.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mglobal_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_global_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraceback_filtering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rlcar/lib/python3.12/site-packages/keras/src/backend/common/global_state.py:16\u001b[0m, in \u001b[0;36mget_global_attribute\u001b[0;34m(name, default, set_to_default)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_global_attribute\u001b[39m(name, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, set_to_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 16\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGLOBAL_STATE_TRACKER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m         attr \u001b[38;5;241m=\u001b[39m default\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "\n",
    "# Training step\n",
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(states, training=True)\n",
    "        \n",
    "        # Get the predicted Q-values for the taken actions\n",
    "        q_values_for_actions = tf.reduce_sum(actions * q_values, axis=1)\n",
    "\n",
    "        next_q_values = model_target(next_states, training=False)\n",
    "        max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "        target_q_values = rewards + (1.0 - dones) * gamma * max_next_q_values\n",
    "\n",
    "        loss = loss_function(target_q_values, q_values_for_actions)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "# Policy function for selecting actions\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Sample continuous actions from the action space\n",
    "    else:\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        q_values = model.predict(state)\n",
    "        return q_values[0]  # Return the continuous action vector\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "frame_count = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = np.array(state)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            action = action_probs[0].numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = np.array(next_state)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(replay_buffer) > batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "            train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "        if frame_count % target_update_freq == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            print(f\"Running reward: {running_reward:.2f} at episode {episode}, frame count {frame_count}\")\n",
    "\n",
    "        if len(replay_buffer) > replay_buffer_size:\n",
    "            replay_buffer.popleft()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        episode_reward_history.pop(0)\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} - Reward: {episode_reward} - Running Reward: {running_reward:.2f} - Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    # Save model every checkpoint_freq episodes\n",
    "    if (episode + 1) % checkpoint_freq == 0:\n",
    "        model.save(f\"car_racing_model_episode_{episode + 1}.keras\")\n",
    "        model_target.save(f\"car_racing_model_target_episode_{episode + 1}.keras\")\n",
    "        print(f\"Checkpoint saved at episode {episode + 1}\")\n",
    "\n",
    "    if running_reward > 40:  # Condition to consider the task solved\n",
    "        print(f\"Solved at episode {episode}!\")\n",
    "        model.save(f\"car_racing_model_solved.h5\")\n",
    "        model_target.save(f\"car_racing_model_target_solved.h5\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video recording function\n",
    "def record_video(env, policy, filename, episode_length=1000):\n",
    "    wrapped_env = RecordVideo(env, video_folder='videos', episode_trigger=lambda x: True, name_prefix=filename)\n",
    "    done = False\n",
    "    state = wrapped_env.reset()\n",
    "    state = np.array(state)\n",
    "    for _ in range(episode_length):\n",
    "        wrapped_env.render()\n",
    "        action = policy(state)\n",
    "        state, reward, done, _, _ = wrapped_env.step(action)\n",
    "        state = np.array(state)\n",
    "        if done:\n",
    "            break\n",
    "    wrapped_env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kmarkwardt/anaconda3/envs/rlcar/lib/python3.12/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/kmarkwardt/gymnasium-car/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Record first episode\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mrecord_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst_episode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mrecord_video\u001b[0;34m(env, policy, filename, episode_length)\u001b[0m\n\u001b[1;32m      4\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m state \u001b[38;5;241m=\u001b[39m wrapped_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 6\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episode_length):\n\u001b[1;32m      8\u001b[0m     wrapped_env\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Sample policy (random actions for now, replace with your policy)\n",
    "def trained_policy(state):\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    q_values = model.predict(state)\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "record_video(env, trained_policy, 'last_episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlcar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
